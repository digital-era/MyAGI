{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/digital-era/MyAGI/blob/main/StableDifusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPocXrQQBp0o"
      },
      "outputs": [],
      "source": [
        "%pip install diffusers accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD5P9FyDCFKo"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
        "pipeline.to(\"cuda\")\n",
        "image = pipeline(\"a photograph of an astronaut riding a horse\").images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZONSkAQEMBn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "7f28c302-ec9e-475a-e562-a592af960a80"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-eab3d6719c16>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
          ]
        }
      ],
      "source": [
        "pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI9P_hi7E1kQ"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
        "scheduler = PNDMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "\n",
        "torch_device = \"cuda\"\n",
        "vae.to(torch_device)\n",
        "text_encoder.to(torch_device)\n",
        "unet.to(torch_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oBL_eSzHsbA",
        "outputId": "7d9a3a41-a0ad-4a43-a45f-915bfd61114f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_size 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
        "height = 512  # default height of Stable Diffusion\n",
        "width = 512  # default width of Stable Diffusion\n",
        "num_inference_steps = 25  # Number of denoising steps\n",
        "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(42)  # Seed generator to create the inital latent noise\n",
        "batch_size = len(prompt)\n",
        "print(\"batch_size\",batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLd6-VjNILb3"
      },
      "outputs": [],
      "source": [
        "\n",
        "text_input = tokenizer(\n",
        "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "print(\"tokenizer.model_max_length=\",tokenizer.model_max_length)\n",
        "print(\"max_length=\",max_length)\n",
        "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "print(\"uncond_input=\",uncond_input)\n",
        "print(\"uncond_embeddings=\",uncond_embeddings)\n",
        "print(\"text_embeddings=\",text_embeddings)\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "print(\"text_embeddings=\",text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG82oeMYKDON"
      },
      "outputs": [],
      "source": [
        "\n",
        "latents = torch.randn(\n",
        "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)\n",
        "\n",
        "latents = latents * scheduler.init_noise_sigma\n",
        "print(\"scheduler.init_noise_sigma=\",scheduler.init_noise_sigma)\n",
        "print(\"latents=\",latents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTLSyP3XLDas"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "def display_denoised_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Denoised Sample @ Step {i}\")\n",
        "    display(image_pil)\n",
        "    return image_pil\n",
        "\n",
        "def display_decoded_image(latents, i):\n",
        "  # scale and decode the image latents with vae\n",
        "  latents = 1 / 0.18215 * latents\n",
        "  with torch.no_grad():\n",
        "    image = vae.decode(latents).sample\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    display(f\"Decoded Image @ step {i}\")\n",
        "    display(pil_images[0])\n",
        "    return pil_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vVNIft_eMPU1"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "denoised_images = []\n",
        "decoded_images = []\n",
        "for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
        "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "    latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
        "\n",
        "    # predict the noise residual\n",
        "    with torch.no_grad():\n",
        "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "    # perform guidance\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    # compute the previous noisy sample x_t -> x_t-1\n",
        "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
        "    if i % 5 == 0:\n",
        "      denoised_image = display_denoised_sample(latents, i)\n",
        "      decoded_image = display_decoded_image(latents, i)\n",
        "      denoised_images.append(denoised_image)\n",
        "      decoded_images.append(decoded_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0YaEfsbRjup"
      },
      "outputs": [],
      "source": [
        "print(latents.shape)\n",
        "latents = 1 / 0.18215 * latents\n",
        "with torch.no_grad():\n",
        "    image = vae.decode(latents).sample\n",
        "    print(image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z85FS7m7RtTD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "device = \"cuda\"\n",
        "model_id_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "image_file = \"/content/sketch-mountains-input.jpg\"\n",
        "\n",
        "init_image = Image.open(image_file).convert(\"RGB\")\n",
        "init_image = init_image.resize((768, 512))\n",
        "\n",
        "prompt = \"A fantasy landscape, trending on artstation\"\n",
        "\n",
        "images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n",
        "\n",
        "display(init_image)\n",
        "display(images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3oo_nimR1SR"
      },
      "outputs": [],
      "source": [
        "prompt = \"ghibli style, a fantasy landscape with castles\"\n",
        "images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n",
        "\n",
        "display(init_image)\n",
        "display(images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc8iMFSLR7P7"
      },
      "outputs": [],
      "source": [
        "prompt = \"ghibli style, a fantasy landscape with castles\"\n",
        "negative_prompt = \"river\"\n",
        "images = pipe(prompt=prompt, negative_prompt=negative_prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n",
        "\n",
        "display(images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gnTVDXekR_Ki"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionUpscalePipeline\n",
        "\n",
        "# load model and scheduler\n",
        "model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
        "pipeline = StableDiffusionUpscalePipeline.from_pretrained(\n",
        "    model_id, revision=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "pipeline = pipeline.to(\"cuda\")\n",
        "\n",
        "# let's download an  image\n",
        "low_res_img_file = \"/content/low_res_cat.png\"\n",
        "low_res_img = Image.open(low_res_img_file).convert(\"RGB\")\n",
        "low_res_img = low_res_img.resize((128, 128))\n",
        "\n",
        "prompt = \"a white cat\"\n",
        "\n",
        "upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\n",
        "\n",
        "low_res_img_resized = low_res_img.resize((512, 512))\n",
        "\n",
        "display(low_res_img_resized)\n",
        "display(upscaled_image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "url = \"https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh\"\n",
        "image_path = \"/content/webui.sh\"\n",
        "urllib.request.urlretrieve(url, image_path)\n",
        "\n",
        "\n",
        "url = \"https://github.com/xuwenhao/geektime-ai-course/raw/main/data/low_res_cat.png\"\n",
        "low_res_img_file = \"/content/low_res_cat.png\"\n",
        "urllib.request.urlretrieve(url, low_res_img_file)\n",
        "\n",
        "url = \"https://github.com/xuwenhao/geektime-ai-course/raw/main/data/sketch-mountains-input.jpg\"\n",
        "image_file = \"/content/sketch-mountains-input.jpg\"\n",
        "urllib.request.urlretrieve(url, image_file)\n",
        "\n",
        "\n",
        "# 对Google Drive进行身份验证\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 将Colab文件保存到您的Google Drive中\n",
        "!cp \"$image_path\" /content/drive/MyDrive/\n",
        "!cp \"$low_res_img_file\" /content/drive/MyDrive/\n",
        "!cp \"$image_file\" /content/drive/MyDrive/\n",
        "#!cp /content/drive/MyDrive/skfigure.xlsx /content\n",
        "\n"
      ],
      "metadata": {
        "id": "qFDrhzi83rSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8e64e6-5410-464a-e881-e46856e58431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!adduser colabuser\n",
        "#!passwd colabuser\n",
        "!sudo su - colabuser\n",
        "\n",
        "#import getpass\n",
        "#username = getpass.getuser()\n",
        "#print(\"当前用户是：\", username)\n",
        "\n",
        "#!chmod +x webui.sh # 添加脚本的执行权限\n",
        "#!./webui.sh # 执行脚本\n"
      ],
      "metadata": {
        "id": "qJiXHyKq-Gjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2J1qIsvSE1U"
      },
      "outputs": [],
      "source": [
        "pipeline.to(\"cuda\")\n",
        "\n",
        "prompt = \"((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\"\n",
        "negative_prompt = \"EasyNegative, extra fingers,fewer fingers,\"\n",
        "image = pipeline(prompt=prompt, negative_prompt=negative_prompt).images[0]\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr6Uz8z/Byfi5R8mKKz19z",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}